{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import LineSentence\n",
    "from corpus import MyTextCorpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluations import evaluate_word_sims, evaluate_word_analogies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lsa import LsaWordVectorizer\n",
    "from hal import HalWordVectorizer\n",
    "from coals import CoalsWordVectorizer\n",
    "from lda import LdaWordVectorizer\n",
    "from word2vec import W2vWordVectorizer\n",
    "from glove import GloveWordVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare corpus\n",
    "\n",
    "sents = LineSentence('data/preprocessed/reuters_sentperline.txt')\n",
    "docs = MyTextCorpus(input = 'data/preprocessed/reuters_docperline.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare corpus\n",
    "\n",
    "sents = LineSentence('data/preprocessed/reuters_sentperline.txt')\n",
    "docs = MyTextCorpus(input = 'data/preprocessed/reuters_docperline.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precomputing L2-norms of word weight vectors\n"
     ]
    }
   ],
   "source": [
    "lsa_wv = LsaWordVectorizer(vector_dim=100, count_normalization='tfidf')\n",
    "lsa_wv.fit_word_vectors(docs.get_text_str())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<hal.HalWordVectorizer at 0x7f124cbb2898>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hal_wv = HalWordVectorizer(token_pattern=r'(?u)\\b\\S+\\b', stop_words = 'english', max_features = 100 )\n",
    "hal_wv.fit_word_vectors(docs.get_text_str())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<coals.CoalsWordVectorizer at 0x7f124af552e8>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coals_wv = CoalsWordVectorizer(window_size = 4,  max_features= 14000, svd_dim =100, token_pattern=r\"(?u)\\b\\S+\\b\")\n",
    "coals_wv.fit_word_vectors(docs.get_text_str())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chien/.pyenv/versions/3.6.2/lib/python3.6/site-packages/gensim/models/ldamodel.py:802: RuntimeWarning: divide by zero encountered in log\n",
      "  diff = np.log(self.expElogbeta)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precomputing L2-norms of word weight vectors\n"
     ]
    }
   ],
   "source": [
    "lda_wv = LdaWordVectorizer(num_topics=100, alpha=0.5, eta=0.01, passes=5)\n",
    "lda_wv.fit_word_vectors('data/preprocessed/reuters_docperline.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbow_wv = W2vWordVectorizer(100, algorithm='cbow', min_count=1)\n",
    "cbow_wv.fit_word_vectors(sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sg_wv = W2vWordVectorizer(100, algorithm='skip-gram', min_count=1)\n",
    "sg_wv.fit_word_vectors(sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/chien/Desktop/NTU/Lab/word-vectors-comparison/GloVe/train.sh --corpus /home/chien/Desktop/NTU/Lab/word-vectors-comparison/data/preprocessed/reuters_docperline.txt --vector_dim 100 --min_count 1 --window_size 5 --max_iter 5 --save_file glove_vectors_reuters_docperline_dim_100_mincount_1_wsize_5_iter_5\n"
     ]
    }
   ],
   "source": [
    "glove_wv = GloveWordVectorizer(100, min_count=1)\n",
    "glove_wv.fit_word_vectors('data/preprocessed/reuters_docperline.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %config Application.log_level=\"INFO\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GLOVE Pearson correlation coefficient against combined.tab: 0.1269\n",
      "GLOVE Spearman rank-order correlation coefficient against combined.tab: 0.1401\n",
      "GLOVE Pairs with unknown words ratio: 16.4%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((0.1268997825527879, 0.029321356217870864),\n",
       " SpearmanrResult(correlation=0.14012239778500016, pvalue=0.016024508615760838),\n",
       " 16.43059490084986)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordsim353 = 'data/evaluations/wordsim353/combined.tab'\n",
    "# evaluate_word_sims(lsa_wv, 'LSA' , wordsim353)\n",
    "# evaluate_word_sims(hal_wv, 'HAL' , wordsim353)\n",
    "# evaluate_word_sims(coals_wv, 'COALS' , wordsim353)\n",
    "# evaluate_word_sims(lda_wv, 'LDA' , wordsim353)\n",
    "# evaluate_word_sims(cbow_wv, 'CBOW' , wordsim353)\n",
    "# evaluate_word_sims(sg_wv, 'SKIP-GRAM' , wordsim353)\n",
    "evaluate_word_sims(glove_wv, 'GLOVE' , wordsim353)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rg = 'data/evaluations/rg_sim.csv'\n",
    "evaluate_word_sims(lsa_wv, 'LSA' , rg, delimiter=';')\n",
    "evaluate_word_sims(hal_wv, 'HAL' , rg, delimiter=';')\n",
    "evaluate_word_sims(coals_wv, 'COALS' , rg, delimiter=';')\n",
    "evaluate_word_sims(lda_wv, 'LDA' , rg, delimiter=';')\n",
    "evaluate_word_sims(cbow_wv, 'CBOW' , rg, delimiter=';')\n",
    "evaluate_word_sims(sg_wv, 'SKIP-GRAM' , rg, delimiter=';')\n",
    "evaluate_word_sims(glove_wv, 'GLOVE' , rg, delimiter=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lsa capital-common-countries: 6.9% (29/420)\n",
      "lsa capital-world: 5.8% (93/1593)\n",
      "lsa currency: 0.0% (0/376)\n",
      "lsa city-in-state: 0.8% (16/1993)\n",
      "lsa family: 0.0% (0/72)\n",
      "lsa gram1-adjective-to-adverb: 0.2% (1/650)\n",
      "lsa gram2-opposite: 0.2% (1/420)\n",
      "lsa gram3-comparative: 1.4% (16/1122)\n",
      "lsa gram4-superlative: 0.8% (4/506)\n",
      "lsa gram5-present-participle: 1.1% (5/462)\n",
      "lsa gram6-nationality-adjective: 29.1% (300/1030)\n",
      "lsa gram7-past-tense: 0.6% (7/1190)\n",
      "lsa gram8-plural: 0.7% (5/702)\n",
      "lsa gram9-plural-verbs: 1.5% (4/272)\n",
      "lsa Quadruplets with out-of-vocabulary words: 44.7%\n",
      "lsa Total accuracy: 4.5% (481/10808)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chien/.pyenv/versions/3.6.2/lib/python3.6/site-packages/numpy/lib/function_base.py:3183: RuntimeWarning: invalid value encountered in true_divide\n",
      "  c /= stddev[:, None]\n",
      "/home/chien/.pyenv/versions/3.6.2/lib/python3.6/site-packages/numpy/lib/function_base.py:3184: RuntimeWarning: invalid value encountered in true_divide\n",
      "  c /= stddev[None, :]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n",
      "(38285,)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-2f0cbe60c4f7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0ma_lsa\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_word_analogies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlsa_wv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'lsa'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'data/evaluations/google_analogies.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrestrict_vocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m300000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcase_insensitive\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdummy4unknown\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0ma_coals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_word_analogies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoals_wv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'coals'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'data/evaluations/google_analogies.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrestrict_vocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m300000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcase_insensitive\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdummy4unknown\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# a_hal = evaluate_word_analogies(hal_wv, 'hal', 'data/evaluations/google_analogies.txt', restrict_vocab=300000, case_insensitive=True, dummy4unknown=False)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# a_lda = evaluate_word_analogies(lda_wv, 'lda', 'data/evaluations/google_analogies.txt', restrict_vocab=300000, case_insensitive=True, dummy4unknown=False)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/NTU/Lab/word-vectors-comparison/evaluations.py\u001b[0m in \u001b[0;36mevaluate_word_analogies\u001b[0;34m(model, name, analogies, restrict_vocab, case_insensitive, dummy4unknown)\u001b[0m\n\u001b[1;32m    185\u001b[0m             \u001b[0;31m# find the most likely prediction using 3CosAdd (vector offset) method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m             \u001b[0;31m# TODO: implement 3CosMul and set-based methods for solving analogies\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m             \u001b[0msims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpositive\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegative\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrestrict_vocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrestrict_vocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m     \u001b[0;31m#         model.vocabulary = original_vocab\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0melement\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msims\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/NTU/Lab/word-vectors-comparison/coals.py\u001b[0m in \u001b[0;36mmost_similar\u001b[0;34m(self, positive, negative, topn, restrict_vocab, indexer)\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m         \u001b[0;31m# limited = self.word_vectors_norm if restrict_vocab is None else self.word_vectors_norm[:restrict_vocab]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 287\u001b[0;31m         \u001b[0msims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_along_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorrcoef\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_vectors_norm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    288\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msims\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.2/lib/python3.6/site-packages/numpy/lib/shape_base.py\u001b[0m in \u001b[0;36mapply_along_axis\u001b[0;34m(func1d, axis, arr, *args, **kwargs)\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[0mbuff\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mind0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mind\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minds\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m         \u001b[0mbuff\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mind\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masanyarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minarr_view\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mind\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/NTU/Lab/word-vectors-comparison/coals.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m         \u001b[0;31m# limited = self.word_vectors_norm if restrict_vocab is None else self.word_vectors_norm[:restrict_vocab]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 287\u001b[0;31m         \u001b[0msims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_along_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorrcoef\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_vectors_norm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    288\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msims\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.2/lib/python3.6/site-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36mcorrcoef\u001b[0;34m(x, y, rowvar, bias, ddof)\u001b[0m\n\u001b[1;32m   3173\u001b[0m         warnings.warn('bias and ddof have no effect and are deprecated',\n\u001b[1;32m   3174\u001b[0m                       DeprecationWarning, stacklevel=2)\n\u001b[0;32m-> 3175\u001b[0;31m     \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcov\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrowvar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3176\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3177\u001b[0m         \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.2/lib/python3.6/site-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36mcov\u001b[0;34m(m, y, rowvar, bias, ddof, fweights, aweights)\u001b[0m\n\u001b[1;32m   3033\u001b[0m         \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3034\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3035\u001b[0;31m     \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mndmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3036\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mrowvar\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3037\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "a_lsa = evaluate_word_analogies(lsa_wv, 'lsa', 'data/evaluations/google_analogies.txt', restrict_vocab=300000, case_insensitive=True, dummy4unknown=False)\n",
    "a_coals = evaluate_word_analogies(coals_wv, 'coals', 'data/evaluations/google_analogies.txt', restrict_vocab=300000, case_insensitive=True, dummy4unknown=False)\n",
    "# a_hal = evaluate_word_analogies(hal_wv, 'hal', 'data/evaluations/google_analogies.txt', restrict_vocab=300000, case_insensitive=True, dummy4unknown=False)\n",
    "# a_lda = evaluate_word_analogies(lda_wv, 'lda', 'data/evaluations/google_analogies.txt', restrict_vocab=300000, case_insensitive=True, dummy4unknown=False)\n",
    "\n",
    "# a_cbow = evaluate_word_analogies(cbow_wv, 'cbow', 'data/evaluations/google_analogies.txt', restrict_vocab=300000, case_insensitive=True, dummy4unknown=False)\n",
    "# a_sg = evaluate_word_analogies(sg_wv, 'skip-gram', 'data/evaluations/google_analogies.txt', restrict_vocab=300000, case_insensitive=True, dummy4unknown=False)\n",
    "# a_glove = evaluate_word_analogies(glove_wv, 'glove', 'data/evaluations/google_analogies.txt', restrict_vocab=300000, case_insensitive=True, dummy4unknown=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora import WikiCorpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sys import getsizeof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_path = 'data/wikipedia/enwiki-20180320-pages-articles2.xml-p30304p88444.bz2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki = WikiCorpus(wiki_path, lemmatize=False, dictionary={})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 10000 articles\n",
      "Processed 20000 articles\n"
     ]
    }
   ],
   "source": [
    "wiki_articles = []\n",
    "i = 0\n",
    "for text in wiki.get_texts():\n",
    "    wiki_articles.append(' '.join(text))\n",
    "#     break\n",
    "        \n",
    "    i +=1\n",
    "    if (i % 10000 == 0):\n",
    "        print(\"Processed \" + str(i) + \" articles\")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "347892335\n"
     ]
    }
   ],
   "source": [
    "# print(len(wiki_articles))\n",
    "# print(sum(map(lambda x: len(x.split()), wiki_articles)))\n",
    "\n",
    "print(sum(map(lambda x: len(x), wiki_articles)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12327451\n",
      "10440255\n"
     ]
    }
   ],
   "source": [
    "print(sum(map(lambda x: len(x.split()), wiki_articles[:5000])))\n",
    "print(sum(map(lambda x: len(x.split()), wiki_articles[5001:10000])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "220.125"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getsizeof(wiki_articles) / 1024 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process InputQueue-12:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/chien/.pyenv/versions/3.6.2/lib/python3.6/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/chien/.pyenv/versions/3.6.2/lib/python3.6/site-packages/gensim/utils.py\", line 1152, in run\n",
      "    wrapped_chunk = [list(chunk)]\n",
      "  File \"/home/chien/.pyenv/versions/3.6.2/lib/python3.6/site-packages/gensim/corpora/wikicorpus.py\", line 579, in <genexpr>\n",
      "    ((text, self.lemmatize, title, pageid, tokenization_params)\n",
      "  File \"/home/chien/.pyenv/versions/3.6.2/lib/python3.6/site-packages/gensim/corpora/wikicorpus.py\", line 355, in extract_pages\n",
      "    elem = next(elems)\n",
      "  File \"/home/chien/.pyenv/versions/3.6.2/lib/python3.6/site-packages/gensim/corpora/wikicorpus.py\", line 349, in <genexpr>\n",
      "    elems = (elem for _, elem in iterparse(f, events=(\"end\",)))\n",
      "  File \"/home/chien/.pyenv/versions/3.6.2/lib/python3.6/xml/etree/ElementTree.py\", line 1223, in iterator\n",
      "    data = source.read(16 * 1024)\n",
      "  File \"/home/chien/.pyenv/versions/3.6.2/lib/python3.6/bz2.py\", line 182, in read\n",
      "    return self._buffer.read(size)\n",
      "  File \"/home/chien/.pyenv/versions/3.6.2/lib/python3.6/_compression.py\", line 68, in readinto\n",
      "    data = self.read(len(byte_view))\n",
      "  File \"/home/chien/.pyenv/versions/3.6.2/lib/python3.6/_compression.py\", line 103, in read\n",
      "    data = self._decompressor.decompress(rawblock, size)\n",
      "OSError: Invalid data stream\n"
     ]
    }
   ],
   "source": [
    "wiki_abs_path = 'data/wikipedia/enwiki-20180120-abstract.xml.gz'\n",
    "wiki_abs = WikiCorpus(wiki_abs_path, lemmatize=False, dictionary={})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import LineSentence, Word2Vec\n",
    "sentences = LineSentence('test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(sentences))\n",
    "model = Word2Vec(sentences, size=100, min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.__contains__('for')\n",
    "model.wv.__getitem__('for')\n",
    "# model.wv.__getitem__('abc')\n",
    "model.wv.similarity('for', 'tool')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in sentences:\n",
    "    print(s)\n",
    "print(getsizeof(sentences) / 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t = '123'\n",
    "# t.seek(0)\n",
    "t = [1,2,3,4,5]\n",
    "t[4:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from word2vec import W2vWordVectorizer\n",
    "from gensim.models.word2vec import LineSentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = W2vWordVectorizer(100, min_count=5)\n",
    "sents = LineSentence('data/preprocessed/reuters_sentperline.txt')\n",
    "w2v.fit_word_vectors(sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.utils\n",
    "import gensim.matutils \n",
    "from scipy import stats\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def evaluate_word_pairs(self, pairs, delimiter='\\t', restrict_vocab=300000,\n",
    "#                             case_insensitive=True, dummy4unknown=False):\n",
    "\n",
    "\n",
    "pairs = 'data/evaluations/wordsim353/combined.tab'\n",
    "delimiter = '\\t'\n",
    "restrict_vocab = 300000\n",
    "case_insensitive = True\n",
    "dummy4unknown = False\n",
    "model = w2v\n",
    "\"\"\"\n",
    "Compute correlation of the model with human similarity judgments. `pairs` is a filename of a dataset where\n",
    "lines are 3-tuples, each consisting of a word pair and a similarity value, separated by `delimiter`.\n",
    "An example dataset is included in Gensim (test/test_data/wordsim353.tsv). More datasets can be found at\n",
    "http://technion.ac.il/~ira.leviant/MultilingualVSMdata.html or https://www.cl.cam.ac.uk/~fh295/simlex.html.\n",
    "The model is evaluated using Pearson correlation coefficient and Spearman rank-order correlation coefficient\n",
    "between the similarities from the dataset and the similarities produced by the model itself.\n",
    "\n",
    "The results are printed to log and returned as a triple (pearson, spearman, ratio of pairs with unknown words).\n",
    "Use `restrict_vocab` to ignore all word pairs containing a word not in the first `restrict_vocab`\n",
    "words (default 300,000). This may be meaningful if you've sorted the vocabulary by descending frequency.\n",
    "If `case_insensitive` is True, the first `restrict_vocab` words are taken, and then case normalization\n",
    "is performed.\n",
    "Use `case_insensitive` to convert all words in the pairs and vocab to their uppercase form before\n",
    "evaluating the model (default True). Useful when you expect case-mismatch between training tokens\n",
    "and words pairs in the dataset. If there are multiple case variants of a single word, the vector for the first\n",
    "occurrence (also the most frequent if vocabulary is sorted) is taken.\n",
    "Use `dummy4unknown=True` to produce zero-valued similarities for pairs with out-of-vocabulary words.\n",
    "Otherwise (default False), these pairs are skipped entirely.\n",
    "\"\"\"\n",
    "ok_vocab = list(model.vocabulary.items())\n",
    "ok_vocab = {w.lower(): v for w, v in ok_vocab} if case_insensitive else dict(ok_vocab)\n",
    "\n",
    "similarity_gold = []\n",
    "similarity_model = []\n",
    "oov = 0\n",
    "\n",
    "# original_vocab = self.vocab\n",
    "# self.vocab = ok_vocab\n",
    "\n",
    "for line_no, line in enumerate(gensim.utils.smart_open(pairs)):\n",
    "    line = gensim.utils.to_unicode(line)\n",
    "    if line.startswith('#'):\n",
    "        # May be a comment\n",
    "        continue\n",
    "    else:\n",
    "        try:\n",
    "            if case_insensitive:\n",
    "                a, b, sim = [word.lower() for word in line.split(delimiter)]\n",
    "            else:\n",
    "                a, b, sim = [word for word in line.split(delimiter)]\n",
    "            sim = float(sim)\n",
    "        except (ValueError, TypeError):\n",
    "            logger.info('Skipping invalid line #%d in %s', line_no, pairs)\n",
    "            continue\n",
    "        if a not in ok_vocab or b not in ok_vocab:\n",
    "            oov += 1\n",
    "            if dummy4unknown:\n",
    "                logger.debug('Zero similarity for line #%d with OOV words: %s', line_no, line.strip())\n",
    "                similarity_model.append(0.0)\n",
    "                similarity_gold.append(sim)\n",
    "                continue\n",
    "            else:\n",
    "                logger.debug('Skipping line #%d with OOV words: %s', line_no, line.strip())\n",
    "                continue\n",
    "        similarity_gold.append(sim)  # Similarity from the dataset\n",
    "        similarity_model.append(model.get_similarity(a, b))  # Similarity from the model\n",
    "# self.vocab = original_vocab\n",
    "spearman = stats.spearmanr(similarity_gold, similarity_model)\n",
    "pearson = stats.pearsonr(similarity_gold, similarity_model)\n",
    "if dummy4unknown:\n",
    "    oov_ratio = float(oov) / len(similarity_gold) * 100\n",
    "else:\n",
    "    oov_ratio = float(oov) / (len(similarity_gold) + oov) * 100\n",
    "\n",
    "logger.debug('Pearson correlation coefficient against %s: %f with p-value %f', pairs, pearson[0], pearson[1])\n",
    "logger.debug(\n",
    "    'Spearman rank-order correlation coefficient against %s: %f with p-value %f',\n",
    "    pairs, spearman[0], spearman[1]\n",
    ")\n",
    "logger.debug('Pairs with unknown words: %d', oov)\n",
    "# print(list(zip(similarity_gold, similarity_model)))\n",
    "print(pearson, spearman, oov_ratio)\n",
    "\n",
    "\n",
    "# self.log_evaluate_word_pairs(pearson, spearman, oov_ratio, pairs)\n",
    "# returnpearson, spearman, oov_ratio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def evaluate_word_analogies(model, analogies, restrict_vocab=300000, case_insensitive=True, dummy4unknown=False):\n",
    "model = glove_wv\n",
    "analogies = 'data/evaluations/google_analogies.txt'\n",
    "restrict_vocab = 300000\n",
    "case_insensitive = True\n",
    "dummy4unknown = False\n",
    "\"\"\"Compute performance of the model on an analogy test set.\n",
    "This is modern variant of :meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.accuracy`, see\n",
    "`discussion on GitHub #1935 <https://github.com/RaRe-Technologies/gensim/pull/1935>`_.\n",
    "The accuracy is reported (printed to log and returned as a score) for each section separately,\n",
    "plus there's one aggregate summary at the end.\n",
    "This method corresponds to the `compute-accuracy` script of the original C word2vec.\n",
    "See also `Analogy (State of the art) <https://aclweb.org/aclwiki/Analogy_(State_of_the_art)>`_.\n",
    "Parameters\n",
    "----------\n",
    "analogies : str\n",
    "    Path to file, where lines are 4-tuples of words, split into sections by \": SECTION NAME\" lines.\n",
    "    See `gensim/test/test_data/questions-words.txt` as example.\n",
    "restrict_vocab : int, optional\n",
    "    Ignore all 4-tuples containing a word not in the first `restrict_vocab` words.\n",
    "    This may be meaningful if you've sorted the model vocabulary by descending frequency (which is standard\n",
    "    in modern word embedding models).\n",
    "case_insensitive : bool, optional\n",
    "    If True - convert all words to their uppercase form before evaluating the performance.\n",
    "    Useful to handle case-mismatch between training tokens and words in the test set.\n",
    "    In case of multiple case variants of a single word, the vector for the first occurrence\n",
    "    (also the most frequent if vocabulary is sorted) is taken.\n",
    "dummy4unknown : bool, optional\n",
    "    If True - produce zero accuracies for 4-tuples with out-of-vocabulary words.\n",
    "    Otherwise, these tuples are skipped entirely and not used in the evaluation.\n",
    "Returns\n",
    "-------\n",
    "(float, list of dict of (str, (str, str, str))\n",
    "    Overall evaluation score and full lists of correct and incorrect predictions divided by sections.\n",
    "\"\"\"\n",
    "\n",
    "ok_vocab = list(model.vocabulary.items()) #restrict_vocab \n",
    "ok_vocab = {w.lower(): v for w, v in ok_vocab} if case_insensitive else dict(ok_vocab)\n",
    "oov = 0\n",
    "\n",
    "# logger.info(\"Evaluating word analogies for top %i words in the model on %s\", restrict_vocab, analogies)\n",
    "\n",
    "sections, section = [], None\n",
    "quadruplets_no = 0\n",
    "for line_no, line in enumerate(gensim.utils.smart_open(analogies)):\n",
    "    line = gensim.utils.to_unicode(line)\n",
    "    if line.startswith(': '):\n",
    "        # a new section starts => store the old section\n",
    "        if section:\n",
    "            sections.append(section)\n",
    "            log_evaluate_word_analogies(section)\n",
    "        section = {'section': line.lstrip(': ').strip(), 'correct': [], 'incorrect': []}\n",
    "    else:\n",
    "        if not section:\n",
    "            raise ValueError(\"Missing section header before line #%i in %s\" % (line_no, analogies))\n",
    "        try:\n",
    "            if case_insensitive:\n",
    "                a, b, c, expected = [word.lower() for word in line.split()]\n",
    "            else:\n",
    "                a, b, c, expected = [word for word in line.split()]\n",
    "        except ValueError:\n",
    "            logger.info(\"Skipping invalid line #%i in %s\", line_no, analogies)\n",
    "            continue\n",
    "        quadruplets_no += 1\n",
    "        if a not in ok_vocab or b not in ok_vocab or c not in ok_vocab or expected not in ok_vocab:\n",
    "            oov += 1\n",
    "            if dummy4unknown:\n",
    "                logger.debug('Zero accuracy for line #%d with OOV words: %s', line_no, line.strip())\n",
    "                section['incorrect'].append((a, b, c, expected))\n",
    "            else:\n",
    "                logger.debug(\"Skipping line #%i with OOV words: %s\", line_no, line.strip())\n",
    "            continue\n",
    "#         original_vocab = model.vocabulary\n",
    "#         model.vocabulary = ok_vocab\n",
    "        ignore = {a, b, c}  # input words to be ignored\n",
    "        predicted = None\n",
    "        # find the most likely prediction using 3CosAdd (vector offset) method\n",
    "        # TODO: implement 3CosMul and set-based methods for solving analogies\n",
    "        sims = model.most_similar(positive=[b, c], negative=[a], topn=5, restrict_vocab=restrict_vocab)\n",
    "#         model.vocabulary = original_vocab\n",
    "        for element in sims:\n",
    "            predicted = element[0].lower() if case_insensitive else element[0]\n",
    "            if predicted in ok_vocab and predicted not in ignore:\n",
    "                if predicted != expected:\n",
    "                    logger.debug(\"%s: expected %s, predicted %s\", line.strip(), expected, predicted)\n",
    "                break\n",
    "        if predicted == expected:\n",
    "            section['correct'].append((a, b, c, expected))\n",
    "        else:\n",
    "            section['incorrect'].append((a, b, c, expected))\n",
    "if section:\n",
    "    # store the last section, too\n",
    "    sections.append(section)\n",
    "    log_evaluate_word_analogies(section)\n",
    "\n",
    "total = {\n",
    "    'section': 'Total accuracy',\n",
    "    'correct': sum((s['correct'] for s in sections), []),\n",
    "    'incorrect': sum((s['incorrect'] for s in sections), []),\n",
    "}\n",
    "\n",
    "oov_ratio = float(oov) / quadruplets_no * 100\n",
    "logger.info('Quadruplets with out-of-vocabulary words: %.1f%%', oov_ratio)\n",
    "# if not dummy4unknown:\n",
    "#     logger.info(\n",
    "#         'NB: analogies containing OOV words were skipped from evaluation! '\n",
    "#         'To change this behavior, use \"dummy4unknown=True\"'\n",
    "#     )\n",
    "analogies_score = log_evaluate_word_analogies(total)\n",
    "sections.append(total)\n",
    "# Return the overall score and the full lists of correct and incorrect analogies\n",
    "# return analogies_score, sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_evaluate_word_analogies(section):\n",
    "        \"\"\"Calculate score by section, helper for\n",
    "        :meth:`evaluate_word_analogies`.\n",
    "        Parameters\n",
    "        ----------\n",
    "        section : dict of (str, (str, str, str, str))\n",
    "            Section given from evaluation.\n",
    "        Returns\n",
    "        -------\n",
    "        float\n",
    "            Accuracy score.\n",
    "        \"\"\"\n",
    "        correct, incorrect = len(section['correct']), len(section['incorrect'])\n",
    "        if correct + incorrect > 0:\n",
    "            score = correct / (correct + incorrect)\n",
    "            logger.info(\"%s: %.1f%% (%i/%i)\", section['section'], 100.0 * score, correct, correct + incorrect)\n",
    "            return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
